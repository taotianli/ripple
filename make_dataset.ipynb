{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bittfgpucuda10conda9ad378efa14b45118fcfa79ad92d1b41",
   "display_name": "Python 3.7.7 64-bit ('tf-gpu-cuda10': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std lib\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# 3rd party lib\n",
    "import h5py\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import resampy\n",
    "\n",
    "#加一个高通滤波\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading file\n",
    "f = h5py.File(\"/p300/PATH_OF_RAW_DATA_LABELED_TEST.h5\",'r')\n",
    "f.keys()# <KeysViewHDF5 ['filenames', 'labels', 'signals']>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = f['labels'][31]\n",
    "#sampling frequency 200Hz\n",
    "#minimum seizure period is 15s\n",
    "#find the start and end time stamp in signal[i]\n",
    "np.max(np.where(a==1))-np.min(np.where(a==1))\n",
    "np.size(np.where(a==1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = [2*0.75/200,2*40/200] #2*0.75Hz/200Hz\n",
    "b, a = signal.butter(6, threshold, 'bandpass')   #配置滤波器 4 表示滤波器的阶数\n",
    "# X = signal.filtfilt(b, a, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find seizure EEG data\n",
    "min_period = 60\n",
    "frequency = 200\n",
    "channel = 18\n",
    "suit_list = list()#suitable_object_list\n",
    "for i in range(500):\n",
    "    signal = f['labels'][i]\n",
    "    if np.size(np.where(signal==1)) != 0:\n",
    "        time_period = np.max(np.where(signal==1))-np.min(np.where(signal==1))\n",
    "        if time_period > min_period*frequency:\n",
    "            suit_list.append([i,np.min(np.where(signal==1)),np.max(np.where(signal==1))])\n",
    "# there are 51 objects which is satisfied conditions in first 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seizure data\n",
    "# eeg_data_list = list()#store the data\n",
    "data_list = list()\n",
    "label_list = list()#zero is normal and one is seizure fragment\n",
    "for tmp_list in suit_list:\n",
    "    i = tmp_list[0]\n",
    "    min_value = tmp_list[1]\n",
    "    max_value = tmp_list[2]\n",
    "    # solving the special data structure array in array\n",
    "    # out_signal = signal.filtfilt(b, a, f['signals'][i])\n",
    "    # out_signal = f['signals'][i]\n",
    "    out = np.concatenate(f['signals'][i]).ravel()\n",
    "    out_reshaped = out.reshape(channel,int(out.size/channel))\n",
    "    out_fragment = out_reshaped[:,min_value:min_value+min_period*frequency]\n",
    "    #change the data to [1,n*18] \n",
    "    out_fragment = np.concatenate(out_fragment).ravel()\n",
    "    # out_fragment = out_fragment.reshape(1,channel*min_period*frequency)\n",
    "    # eeg_data_list.append(out_fragment)\n",
    "    data_list.append(out_fragment)\n",
    "    label_list.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find normal EEG data\n",
    "min_period = 60\n",
    "frequency = 200\n",
    "channel = 18\n",
    "norm_suit_list = list()#suitable_object_list\n",
    "for i in range(500):\n",
    "    signal = f['labels'][i]\n",
    "    if np.size(np.where(signal==1)) == 0:\n",
    "        time_period = np.max(np.where(signal==0))-np.min(np.where(signal==0))\n",
    "        if time_period > min_period*frequency:\n",
    "            norm_suit_list.append([i,np.min(np.where(signal==0)),np.max(np.where(signal==0))])\n",
    "# there are 23 objects which is satisfied conditions in first 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normal data\n",
    "# normal_data_list = list()#store the data\n",
    "for tmp_list in norm_suit_list:\n",
    "    i = tmp_list[0]\n",
    "    min_value = tmp_list[1]\n",
    "    max_value = tmp_list[2]\n",
    "    # solving the special data structure array in array\n",
    "    out = np.concatenate(f['signals'][i]).ravel()\n",
    "    out_reshaped = out.reshape(channel,int(out.size/channel))\n",
    "    out_fragment = out_reshaped[:,min_value:min_value+min_period*frequency]\n",
    "    out_fragment = np.concatenate(out_fragment).ravel()\n",
    "    # out_fragment = out_fragment.reshape(1,channel*min_period*frequency)\n",
    "    # eeg_data_list.append(out_fragment)\n",
    "    data_list.append(out_fragment)\n",
    "    label_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "def Normalization(x):\n",
    "    return 10*np.array(x)/np.mean(x)\n",
    "for i in range(len(data_list)):\n",
    "    data_list[i] = Normalization(data_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the data as .mat file for matlab\n",
    "import scipy.io as scio\n",
    "# file_path_1 = 'normal_data_test.mat'\n",
    "# file_path_2 = 'seizure_data_test.mat'\n",
    "# scio.savemat(file_path_1, {'Normal_EEG':normal_data_list})\n",
    "# scio.savemat(file_path_2, {'Seizure_EEG':seizure_data_list})\n",
    "\n",
    "file_path_1 = 'data_test.mat'\n",
    "scio.savemat(file_path_1, {'Data':data_list})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "#loading data\n",
    "datafile = 'data_test.mat'\n",
    "# data = scio.loadmat(datafile)\n",
    "data = scio.loadmat(datafile)['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SVM with different variables\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split as ts\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#import our data\n",
    "\n",
    "# X = np.nan_to_num(data['final_feature'])\n",
    "X = data_list\n",
    "y = label_list\n",
    "\n",
    "X,y = shuffle(X,y,random_state=0)\n",
    "# X_1,X_test,y_1,y_test = ts(X,y,test_size=0.3) #分成两份，一份train&eval 一份val\n",
    "X_train,X_test,y_train,y_test = ts(X,y,test_size=0.3) \n",
    "# select different type of kernel function and compare the score\n",
    "param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]} \n",
    "\n",
    "grid_search = GridSearchCV(svm.SVC(), param_grid, cv=2, scoring='accuracy' )    \n",
    "grid_search.fit(X_train,y_train)\n",
    "best_parameters = grid_search.best_estimator_.get_params()    \n",
    "model = svm.SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)    \n",
    "model.fit(X_train,y_train)  \n",
    "pred = model.predict(X_test)\n",
    "score = model.score(X_test,y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM with different variables\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split as ts\n",
    "from sklearn.utils import shuffle\n",
    "#import our data\n",
    "\n",
    "# X = np.nan_to_num(data['final_feature'])\n",
    "X = data_list\n",
    "y = label_list\n",
    "\n",
    "X,y = shuffle(X,y,random_state=0)\n",
    "# X_1,X_test,y_1,y_test = ts(X,y,test_size=0.3) #分成两份，一份train&eval 一份val\n",
    "X_train,X_test,y_train,y_test = ts(X,y,test_size=0.6) \n",
    "# select different type of kernel function and compare the score\n",
    "\n",
    "\n",
    "# kernel = 'rbf'\n",
    "clf_rbf = svm.SVC(kernel='rbf')\n",
    "clf_rbf.fit(X_train,y_train)\n",
    "clf_rbf_pred = clf_rbf.predict(X_test)\n",
    "score_rbf = clf_rbf.score(X_test,y_test)\n",
    "print(\"The score of rbf is : %f\"%score_rbf)\n",
    "\n",
    "# kernel = 'linear'\n",
    "clf_linear = svm.SVC(kernel='linear')\n",
    "clf_linear.fit(X_train,y_train)\n",
    "clf_linear_pred = clf_linear.predict(X_test)\n",
    "score_linear = clf_linear.score(X_test,y_test)\n",
    "print(\"The score of linear is : %f\"%score_linear)\n",
    "\n",
    "# kernel = 'poly'\n",
    "clf_poly = svm.SVC(kernel='poly')\n",
    "clf_poly.fit(X_train,y_train)\n",
    "clf_poly_pred = clf_poly.predict(X_test)\n",
    "score_poly = clf_poly.score(X_test,y_test)\n",
    "print(\"The score of poly is : %f\"%score_poly)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "single column \n",
    "The score of rbf is : 0.657143\n",
    "The score of linear is : 0.628571\n",
    "The score of poly is : 0.542857\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM & Random Forest & Logistic regression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X,y = shuffle(X,y,random_state=0)\n",
    "X_train,X_test,y_train,y_test = ts(X,y,test_size=0.3) \n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr',log_clf),('rf',rnd_clf),('svc',svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train,y_train)\n",
    "\n",
    "for clf in (log_clf,rnd_clf,svm_clf):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__,accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN clssifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X,y = shuffle(X,y,random_state=0)\n",
    "X_train,X_test,y_train,y_test = ts(X,y,test_size=0.3) \n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2)\n",
    "knn_clf.fit(X_train,y_train)\n",
    "y_pred = knn_clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    " \n",
    "# 全局变量\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "epochs = 12\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "input_shape = (1, img_rows, img_cols)\n",
    "\n",
    "#构建模型\n",
    "model = Sequential()\n",
    "\"\"\"\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='same',\n",
    "                        input_shape=input_shape))\n",
    "\"\"\"\n",
    "model.add(Convolution2D(nb_filters, (kernel_size[0], kernel_size[1]),\n",
    "                        padding='same',\n",
    "                        input_shape=input_shape)) # 卷积层1\n",
    "model.add(Activation('relu')) #激活层\n",
    "model.add(Convolution2D(nb_filters, (kernel_size[0], kernel_size[1]))) #卷积层2\n",
    "model.add(Activation('relu')) #激活层\n",
    "model.add(MaxPooling2D(pool_size=pool_size)) #池化层\n",
    "model.add(Dropout(0.25)) #神经元随机失活\n",
    "model.add(Flatten()) #拉成一维数据\n",
    "model.add(Dense(128)) #全连接层1\n",
    "model.add(Activation('relu')) #激活层\n",
    "model.add(Dropout(0.5)) #随机失活\n",
    "model.add(Dense(nb_classes)) #全连接层2\n",
    "model.add(Activation('softmax')) #Softmax评分\n",
    " \n",
    "#编译模型\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "#训练模型\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(X_test, Y_test))\n",
    "#评估模型\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "threshold = [2*0.75/200,2*40/200] #2*0.75Hz/200Hz\n",
    "b, a = signal.butter(4, threshold, 'bandpass')   #配置滤波器 4 表示滤波器的阶数\n",
    "X = signal.filtfilt(b, a, X)"
   ]
  }
 ]
}